# 2025-07-28

aws command line interface is a quite confusing at first grance. There are many
subcommands and some of them works under specific conditions. For example, most
subcommands are affected by the current region which is set by ~/.aws/config and
some of them require an option like --cluster or --task-definition instead of
position argument.

```
$ aws ecs list-services --cluster zwls-api-tokyo-cluster
{
    "serviceArns": [
        "arn:aws:ecs:ap-northeast-1:649282530217:service/zwls-api-tokyo-cluster/z-emotion-api-dev-service",
        "arn:aws:ecs:ap-northeast-1:649282530217:service/zwls-api-tokyo-cluster/z-emotion-api-production-service"
    ]
}

$ aws ecs list-tasks --cluster zwls-api-tokyo-cluster
{
    "taskArns": [
        "arn:aws:ecs:ap-northeast-1:649282530217:task/zwls-api-tokyo-cluster/2d0f61b86387434e96639a0b34951b9b",
        "arn:aws:ecs:ap-northeast-1:649282530217:task/zwls-api-tokyo-cluster/a0daae0a0bb74e9daa915507ca12023c"
    ]
}

$ aws ecs list-task-definition-families
{
    "families": [
        "elastic-TaskDefinition",
        "elastic-search-TaskDefinition",
        "z-emotion-admin-tokyo-TaskDefinition",
        "z-emotion-api-tokyo-TaskDefinition",
        "zwls-api-task-def",
        "zwls-api-tokyo-TaskDefinition"
    ]
}

$ aws ecs describe-task-definition --task-definition z-emotion-api-tokyo-TaskDefinition:664

{
    "taskDefinition": {
        "taskDefinitionArn": "arn:aws:ecs:ap-northeast-1:649282530217:task-definition/z-emotion-api-tokyo-TaskDefinition:664",
        "containerDefinitions": [...],
        ...
    }
}

$ aws ecs describe-services --services 'z-emotion-api-dev-service' --cluster zwls-api-tokyo-cluster
{
    "services": [
        {
            "serviceArn": "arn:aws:ecs:ap-northeast-1:649282530217:service/zwls-api-tokyo-cluster/z-emotion-api-dev-service",
            "serviceName": "z-emotion-api-dev-service",
            "clusterArn": "arn:aws:ecs:ap-northeast-1:649282530217:cluster/zwls-api-tokyo-cluster",
            ...
        }
        ...
    ],
    ...
}
```

By default, aws prints an output as json. You can change the output format by
setting `output` option in the profile file.

# 2025-07-27

I copyied my AWS access key to WSL and olivine.

There are snap package for aws-cli but it doesn't support aws_completer. So I
decided to install it from source AWS provides. I could install it in my local
directory by giving -i and -b option like below:

```
aws/install -i ~/.local/aws-cli -b ~/.local/bin
```

# 2025-07-25

I found why the version string from /hello didn't match to `git describe master`.
When running `git describe` in GitHub Action, it gives `HEAD~` instead of
`HEAD`, `version=$(git describe --tags --match 'v*' --exclude '*-rc*' HEAD~)`.
I don't know why it does that way.

--------------------------------------------------------------------------------

I could fix 502 Bad Gateway error. I was inspecting the log by
`aws logs tail /ecs/z-emotion-api-tokyo-TaskDefinition`. And there was an error
message, like:

```
/usr/src/app/node_modules/@slack/webhook/dist/errors.js:36
    const error = errorWithCode(new Error(`An HTTP protocol error occurred: statusCode = ${original.response.status}`), ErrorCode.HTTPError);
                                ^
Error: An HTTP protocol error occurred: statusCode = 404
    at httpErrorWithOriginal (/usr/src/app/node_modules/@slack/webhook/dist/errors.js:36:33)
    at IncomingWebhook.send (/usr/src/app/node_modules/@slack/webhook/dist/IncomingWebhook.js:55:58)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async /usr/src/app/dist/apps/api/main.js:5878:13 {
  code: 'slack_webhook_http_error',
  original: Error: Request failed with status code 404
      at createError (/usr/src/app/node_modules/@slack/webhook/node_modules/axios/lib/core/createError.js:16:15)
      at settle (/usr/src/app/node_modules/@slack/webhook/node_modules/axios/lib/core/settle.js:17:12)
      at IncomingMessage.handleStreamEnd (/usr/src/app/node_modules/@slack/webhook/node_modules/axios/lib/adapters/http.js:269:11)
      at IncomingMessage.emit (node:events:536:35)
      at endReadableNT (node:internal/streams/readable:1698:12)
      at process.processTicksAndRejections (node:internal/process/task_queues:82:21) {
    config: {
      url: 'https://hooks.slack.com/services/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',
      method: 'post',
      data: '{"timeout":0,"attachments":[{"color":"#05bc79","blocks":[{"type":"section","text":{"type":"mrkdwn","text":"[production] You have a new request."}},
      {"type":"section","text":{"type":"mrkdwn","text":"*When:*\\nFri Jul 25 2025 08:35:49 GMT+0000 (Coordinated Universal Time)"}},
      {"type":"divider"},
      {"type":"section","fields":[{"type":"plain_text","text":"Type:\\n:baby::skin-tone-2: userCreated","emoji":true},
      {"type":"mrkdwn","text":"*fullname:*\\nKyle Brownlee"},
      {"type":"mrkdwn","text":"*email:*\\nkyle.hong+alias11@gmail.com"},
      {"type":"mrkdwn","text":"*company:*\\nnull"}
      ]}]}]}',
      headers: {
        Accept: 'application/json, text/plain, */*',
        'Content-Type': 'application/json',
        'User-Agent': '@slack:webhook/6.1.0 node/20.19.4 linux/5.10.238-234.956.amzn2.x86_64',
        'Content-Length': 570
      },
      proxy: false,
      baseURL: 'https://hooks.slack.com/services/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',
      transformRequest: [ [Function: transformRequest] ],
      transformResponse: [ [Function: transformResponse] ],
      timeout: 0,
      adapter: [Function: httpAdapter],
      xsrfCookieName: 'XSRF-TOKEN',
      xsrfHeaderName: 'X-XSRF-TOKEN',
      maxContentLength: -1,
      maxBodyLength: -1,
      maxRedirects: 0,
      validateStatus: [Function: validateStatus],
      transitional: {
        silentJSONParsing: true,
        forcedJSONParsing: true,
        clarifyTimeoutError: false
      }
    },
    ...
}
```

So I was thinking there was a try to send Slack nofications when new user sign
up and suddenly Slack stopped to support that web hook. I removed the Slack
notification code and 502 Bad gateway error had gone. But the server is still
failing to send verification emails.

# 2025-07-24

I ran localhost API server and call REST API to test the signup routine, but
nothing went wrong. Interestingly, the version strings from /hello didn't match
ones I got by `git describe`.

Versions:
prod/hello      v0.0.2-1495-g8f01cda6
dev/hello       v0.0.2-1493-gdb9e838b
master branch   v0.0.2-1498-g23f205d9
dev branch      v0.0.2-1494-g745d7d0b

I switched to db9e838b, the same commit as dev, and tried to signup, but it
failed. It doesn't look like commit matters. Something in infrastructure or
external services masterr, I guess.

---------------------------------------------------------------------

Docker Desktop ran into an unexpected WSL error. Here's the error message:

```
An unexpected error occurred while executing a WSL command.

Either shut down WSL down with wsl --shutdown, and/or reboot your machine.
You can also try reinstalling WSL and/or Docker Desktop.
If the issue persists, collect diagnostics and submit an issue.

running wsl-bootstrap: running WSL command wsl.exe C:\WINDOWS\System32\wsl.exe -d docker-desktop
-u root -e wsl-bootstrap run --base-image /c/Program Files/Docker/Docker/resources/docker-desktop.iso
--cli-iso /c/Program Files/Docker/Docker/resources/wsl/docker-wsl-cli.iso
--data-disk b675d3ed-7307-714d-8f0b-554606b681cd: getty: cmdline has console=hvc0 but does not exist in /etc/securetty;
will not be able to log in as root on this tty hvc0.
: exit status 1
```

As I reinstalled Docker Desktop, it looked like the issues has gone. But I don't
know it has been exactly resolved.

# 2025-07-23

Blair complained that she was not able to copy asset id by clicking the id link
in asset store. Actually, it works but she connected directly to S3 bucket via
http://z-backoffice-v2.s3-website.ap-northeast-2.amazonaws.com/login, not
https://di6vhzh0u9tr9.cloudfront.net/login.

[Clipboard API](https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/Interact_with_the_clipboard#writing_to_the_clipboard)
only works on https. That's why she wasn't able to copy ids.

---------------------------------------------------------------------

when I tried to signup to z-emotion.com I got the following error and failed to sign up.


```
XHR POST https://api.z-emotion.com/prod/v2/user/signup CORS Missing Allow Origin


POST                https://api.z-emotion.com/prod/v2/user/signup
Status              502
Version             HTTP/2
Transferred         260 B (122 B size)
Referrer Policy     strict-origin-when-cross-origin
Request Priority    Highest
DNS                 ResolutionSystem

Response Headers
content-length      122
content-type        text/html
date                Wed, 23 Jul 2025 05:57:49 GMT
server              awselb/2.0
X-Firefox-Spdy      h2

Request Headers
Accept              application/json, text/plain, */*
Accept-Encoding     gzip, deflate, br, zstd
Accept-Language     en-US,en;q=0.5
Authorization       Bearer undefined
Connection          keep-alive
Content-Length      158
Content-Type        application/json
Host                api.z-emotion.com
Origin              https://z-emotion.com
Priority            u=0
Referer             https://z-emotion.com/
Sec-Fetch-Dest      empty
Sec-Fetch-Mode      cors
Sec-Fetch-Site      same-site
TE                  trailers
User-Agent          Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:140.0) Gecko/20100101 Firefox/140.0
withCredentials     true

Request

{
    "email":"nq2f6ebm6@mozmail.com",
    "password": ***********,
    "firstname":"Why do",
    "lastname":"you want",
    "company":"know my name?",
    "loginType":"credentials"
}

Response
<html>
<head><title>502 Bad Gateway</title></head>
<body>
<center><h1>502 Bad Gateway</h1></center>
</body>
</html>


Cross-Origin Request Blocked: The Same Origin Policy disallows reading the remote resource at https://api.z-emotion.com/prod/v2/user/signup.
(Reason: CORS header ‘Access-Control-Allow-Origin’ missing). Status code: 502.
```

# 2025-07-22

I ran `npm run build` in Vitarelax directory on WSL and I got out of memory
error.

```
  FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed - JavaScript heap out of memory
```

I could solve this memory issue by [setting NODE_OPTIONS environment
variable](https://stackoverflow.com/a/59572966).

And I found that there are two versions of Babylon.js in Vitarelax project. The
one is Wumia's and the others Vitarelax's. Actually, Vitarelax has nothing to do
with Babylon.js directly. It made me think about the different behaviors on
Windows and WSL. For some reason, it looked like `npm run dev` on WSL selected
the wrong version of Babylon.js. I removed the dependency of Vitarelax on
Babylon.js to verify my guess. And yes. It worked as I remove the dependency.

------------------------------------------------------------------------------

When I was creating the EC2 instance, I only allowed firewall to except traffics
from my IP. That's why AWS Connect didn't work. I could modify that setting by
editing security group assigned to the instance.

-------------------------------------------------------------------------------

I installed nginx on the instance and I could connect to it(15.164.213.38) using a web broswer.

# 2025-07-21

I installed nginx on WSL. I could connect to localhost using a web browser.

--------------------------------------------------------------------------------

I created an EC2 instance, but I could connect to it via SSH
(ubuntu@15.164.213.38), but Amazon Connect didn't work. ChatGPT said I need to
apply a role to the instance and I followed the guideline, but it didn't work.

# 2025-07-20

15 July, I tried to debug a Razor app in WSL from VSCode, but it didn't work. I
still don't know why. So I installed dotnet SDK and created a console app on
Windows. And then install C# Dev Kit for VS Code. At first, I couldn't debugging
it. It complains that dotnet is not recognized as an internal or external
command. After close and relaunching all VS Code instances, it suddenly worked.

I did the same thing one more time, but instead I created a console app from
WSL. I could debug it from VS Code as well. After all I could debug my Razor
app. I don't know what made it impossible to debug and what made it possible
again. Maybe it's just about the order of plugin installation or PATH problem.

# 2025-07-17

I had to check if there are mismatches between our database and the excel file Audaces gave us.
We don't have web UI to check it so I need to run SQL manually. 4 people were missing from the Audaces team.

# 2025-07-16

I had to build a 360 panorama showroom demo for Hyosung. I decided to use [Photo
Sphere Viewer](https://photo-sphere-viewer.js.org/). I got started from a demo
it provided. It just uses script tags instead of NPM packages. I could've
changed it to get built by vite, but I thought I had no time to setup. I don't
know if it was the right decision.

# 2025-07-15

I tried to run Vitarelax in WSL. I feteched the source, initialize, and update
Wumia. `npm run dev` seemed to work, but I got a runtime error when I opened a
model page.

```typescript wumia-core.ts
this.props.noSkybox = this.props.noBg || this.props.noSkybox;
// console.log("Wumia", "version", import.meta.env.VITE_APP_VERSION);
this.engine = scene.getEngine() as Engine;
this.canvas = this.engine.getRenderingCanvas()!;
scene.createDefaultCamera(true, true, true); // createDefaultCamera is undefined!
```

I don't know why the behavior is different from when I run it on Windows. Maybe
there are some other project have the same issue, I guess.

---------------------------------------------------------------

I installed WSL extension for VSCode, ran `code .` in WSL, and got the following
message.

```
Installing VS Code Server for Linux x64 (cb0c47c0cfaad0757385834bd89d410c78a856c0)
Downloading: 100%
Unpacking: 100%
Unpacked 2050 files and folders to /home/kyle/.vscode-server/bin/cb0c47c0cfaad0757385834bd89d410c78a856c0.
Looking for compatibility check script at /home/kyle/.vscode-server/bin/cb0c47c0cfaad0757385834bd89d410c78a856c0/bin/helpers/check-requirements.sh
Running compatibility check script
Compatibility check successful (0)
```

VSCode opened and there was a WSL indicator at lower left corner. I could
navigate files and open a terminal just like I was in WSL. I could access files
in WSL from File Explorer by going `\\wsl.localhost\`.

# 2025-07-13


This's the table I could use when ssh-ing.

| Instance                      | Credential         | User     | IP             |
| ----------------------------- | ------------------ | -------- | -------------- |
| zelus-ci-linux-1              | zelus-ci-linux.pem | ubuntu   | 3.35.165.240   |
| zelus-ci-linux-2              | zelus-ci-linux.pem | ubuntu   | 3.38.137.232   |
| zelus-ci-linux-3 (CUDA)       | zelus-ci-linux.pem | ubuntu   | 3.38.111.252   |
| ssh-tunnel-ze-db              | zw-license-key.pem | ec2-user | 3.36.83.177    |
| zwls-prod-seoul-vpc-vpn       | zw-license-key.pem | ec2-user | 13.124.100.90  |
| zwls-prod-tokyo-vpc-vpn       | zw-license-key.pem | ec2-user | 13.113.226.89  |
| elastic-search-kibana-tokyo   | zw-license-key.pem | ec2-user | 13.115.161.148 |
| elastic-search-kibana-tokyo   | zw-license-key.pem | ec2-user | 13.115.161.148 |
| ssh-tunnel-pixel-db           | z-pixel            | ubuntu   | 13.125.228.208 |

I couldn't connect to ssh-tunnel-pixel-db with z-pixel.pem that was downloaded
from [notion page](https://www.notion.so/zemotion/keys-e066d9ce81c14129a97ce9bd52654e0f).


`ssh -i ~/secrets/z-pixel.pem ec2-user@13.125.228.208` didn't work with an error
message `ec2-user@13.125.228.208: Permission denied (publickey).` So I wanted to
check if the z-pixel.pem was actually came from AWS. To verify it I needed to
check the fingerprint of it.

```bash
# AWS dashboard says fingerprint is a6:9a:db:93:e4:50:39:98:b6:7f:6b:67:f2:ba:24:26:0a:f0:69:75
ssh-keygen -yf ~/secrets/z-pixel.pem >~/secrets/z-pixel.pub
# This prints 2048 MD5:b3:b5:e7:a1:17:45:a9:4b:15:7e:75:2b:f1:38:48:17 no comment (RSA)
ssh-keygen -lf ~/secrets/z-pixel.pem -E MD5
```

[Stackoverflow](https://serverfault.com/a/603983) says there are at least two
ways of generating fingerprints, 1) ssh-keygen and 2) openssl. And AWS
distingishes keys that are imported from local machines and are generated from
AWS console. AWS generates the SHA1 fingerprint from the private key if they are
generated from AWS. So I ran the following command.

```bash
# This prints SHA1(stdin)= a6:9a:db:93:e4:50:39:98:b6:7f:6b:67:f2:ba:24:26:0a:f0:69:75
openssl pkcs8 -in ~/secrets/z-pixel.pem -nocrypt -topk8 -outform DER | openssl sha1 -c
```

Here's [the official guide](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/verify-keys.html)
to verify the fingerprint.

It exactly matches the one on the dashboard. The key was right, but the user
name was not. `ubuntu` was the right user name for ssh-tunnel-pixel-db.

# 2025-07-21

I learned how to implement dependent dropdown lists in Google Spreadsheet. It
makes use of FILTER and TRANSPOSE function and data validation rules.
Interestingly, a formula in a cell can set values of other cells. It is not
limited to change its own value. And I could make use of it while creating a
dependent dropdown.

# 2025-07-11

I changed the measurement.json format to make it possible to insert additional
anchors into the scene without modifying glb files.

-----------------------------------------------------------------

I run docker as root from z-vto directory.
`docker run -v ".:/workspace" -p "8080:8080" --gpus all -it 54b94ee24e0c`.

To install dependencies I ran `python -m pip install -r requirements.txt`, but I
got following errors:

```
ERROR: pip's dependency resolver does not currently take into account all the packages that are installed.
This behaviour is the source of the following dependency conflicts.
bentoml 1.2.19 requires nvidia-ml-py<12, but you have nvidia-ml-py 12.575.51 which is incompatible.
sagemaker 2.233.0 requires cloudpickle==2.2.1, but you have cloudpickle 3.0.0 which is incompatible.
```

I ignored the error and run `python run_gradio_union.py` Python couldn't find
detectron2 module. It turned out that detectron2 and annotator was broken links.
I should remove them and copy the directory from NAS. After copying files. I can
run pip without errors, but_gradio_union.py gave another error.

```
FileNotFoundError: [Errno 2] No such file or directory:
'/home/nas4_user/jeonghokim/Hoeyeong_Jin/VITON/Z-emotion-project/z-vto/logs/best_checkpoints/20241030_DCupper_DClower_DCdress_agnoffihand_agnoffiv9hand_rand dilate_gpt4ov6v12_jointcond_nopose_noip_intermcloth_resumev3v7_180kiter/checkpoint-240000_0.0330/pytorch_model.bin'
```

I couldn't find the directory in NAS. So I replace it with
`./logs/z-emotion-man/checkpoint-1200_0.0504/pytorch_model.bin` instead. It
worked after replacing the path.


# 2025-07-10

I added my account to docker group by `usermode -a -G docker $USER` in olivine,
but groups doesn't tell that I'm in the docker group even thought /etc/group
tells it is. Apperently `usermod` doesn't affect the current session, so I
logged out and in and it worked as expected.

After pulling an image I tried to run a container by
`docker run -v ".:/workspace" -p "8080:8080" --gpus all -it <image-id>`, but I
got an error like below.

```
docker: Error response from daemon: could not select device driver "" with capabilities: [[gpu]].
```

[Installing the nVidia Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)

```bash
curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \
    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list

sudo apt-get update
```

`apt search nvidia-container-toolkit` might list nvidia packages. Even though
systemctl says nvidia-powerd.service was loaded but failed to run, no need to
worry. It is for specific Optimus-based laptops and if your system just uses
nVidia GPUs, it does nothing. You can disable it.


```bash
sudo nvidia-ctk runtime configure --runtime=docker
sudo systemctl restart docker

```

To run docker in rootless mode, [you need to install uidmap package](https://docs.docker.com/engine/security/rootless/).

```bash
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | \
    sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt update
sudo install uidmap docker-ce-rootless-extras
```

`dockerd-rootless-setuptool.sh install` failed even though I gave `--force option`.
`journalctl --user -xeu docker.service` says:

```
Jul 10 13:14:49 olivine dockerd-rootless.sh[6127]: running with RootlessKit, but rootlesskit-docker-proxy not installed: exec: "rootlesskit-docker-proxy": executable file not found in $PATH
Jul 10 13:14:49 olivine dockerd-rootless.sh[6099]: [rootlesskit:child ] error: command [/usr/bin/dockerd-rootless.sh] exited: exit status 1
Jul 10 13:14:49 olivine dockerd-rootless.sh[6087]: [rootlesskit:parent] error: child exited: exit status 1
Jul 10 13:14:49 olivine systemd[3194]: docker.service: Main process exited, code=exited, status=1/FAILURE
```

rootlesskit-docker-proxy was indeed not found. I think I'd better run docker as
root and test z-vto for now and run rootless later.


# 2025-07-09

I needed to copy files in the NAS to linux machine. I could mount SMB share to
my local directory by
`sudo mount -t cifs -o credentials=~/.smbcredentials //<ip-address>/<directory> <mount-point>`.
[I needed to install cifs-utils before mounting](https://askubuntu.com/a/923428).

# 2025-07-08

Vitarelax doesn't version control sofa and fabric files. They are consist of
glb, png, and other binary files. I decided to add them to Git for better
collaboration experience. It takes up a lot of space but it's not that huge that
Git can't control.

# 2025-07-02

I found that lb means a load balancer, but I can't remember where it was used
and why I was curious about it.

# 2025-07-01

I took a look at what EC2 instances was there and what key they was using.
Here's a list of them.

```
ap-northeast-2
Id                                                Key Name            Key Created Date                        Key Id                Public IP       Working
i-00f4630754249128e (zwls-prod-seoul-vpc-vpn)     zw-license-key      2021/01/27 29:38:...:8a:60             key-07f8b708c200109b8 13.124.100.90    Yes
i-09d4166421a56e775 (ssh-tunnel-ze-db)            zw-license-key      2021/01/27 29:38:...:8a:60             key-07f8b708c200109b8 3.36.83.177      Yes
i-01b16fa0d4081cdda (zelus-ci-windows-1)          zelus-ci-windows    2022/07/13 5c:a3:...:4f:96:84:7a:07:28 key-0f098a158d7ad08cb 52.78.15.171     Yes
i-0f286f84cf956ed46 (zelus-ci-linux-2)            zelus-ci-windows    2022/07/13 5c:a3:...:4f:96:84:7a:07:28 key-0f098a158d7ad08cb 3.38.137.232     Yes
                                                  zelus-ci            2022/07/12 7a:95:...:0d:47:88:7e:09:2a key-0f39e9a6b18581daf
i-0a58e23b4aa4161f6 (zelus-ci-linux-1)            zelus-ci-windows    2022/07/13 5c:a3:...:4f:96:84:7a:07:28 key-0f098a158d7ad08cb 3.35.165.240     Yes
                                                  zelus-ci            2022/07/12 7a:95:...:0d:47:88:7e:09:2a key-0f39e9a6b18581daf
i-00c284f8510d2d952 (zelus-ci-linux-3 (CUDA))     zelus-ci-windows    2022/07/13 5c:a3:...:4f:96:84:7a:07:28 key-0f098a158d7ad08cb 3.38.111.252     Yes
                                                  zelus-ci            2022/07/12 7a:95:...:0d:47:88:7e:09:2a key-0f39e9a6b18581daf
i-0119b08adf309e453 (ssh-tunnel-pixel-db)         z-pixel             2023/10/23 a6:9a:...:24:26:0a:f0:69:75 key-0648ad85a63f288ff 13.125.228.208   Yes
i-04c372f5d6c3a4047 (prometheus-runner)           general-ec2-key     2024/06/24 3a:55:...:89:11:66:8e:bf:22 key-0bab30d0c01b5f656 -                No
i-06697a9e98862806f (znest-api)                   cred-znest-window   2024/09/05 fb:d9:...:05:4c:66:67:36:65 key-0c5ba086cb62f201f -                No
i-0407f18850e213ceb (apg-pdf-handler)             apg-pdf-test-server 2024/09/03 6d:68:...:1d:4e:8b:b3:3a:9c key-0efb5336d5581636f -                No
i-01ee5e8663eea5e5f (zstudio-be)                  zstudio-be          2024/08/27 8e:d9:...:84:90:9f:81:9a:70 key-041de7c2501dc0c88 -                No
i-0743d0ff992646258 (z-virtual)                   z-virtual           2024/11/22 e7:b4:...:0b:47:b4:99:9e:6e key-0e523d6a8f3993fa2 54.180.57.152    No
i-05c07c86465fd9ec9                               z-pixel             2023/10/23 a6:9a:...:24:26:0a:f0:69:75 key-0648ad85a63f288ff -                No

ap-northest-1
Id                                                Key Name            Key Created Date                       Key Id                Public IP
i-0c98c7c727d93540c (zwls-prod-tokyo-vpc-vpn)     zw-license-key      2021/08/17 29:38:...:df:d4:f1:72:8a:60 key-0e07798a956b7d0af 13.113.226.89    Yes
i-07db139c64a35af45 (elastic-search-kibana-tokyo) zw-license-key      2021/08/17 29:38:...:df:d4:f1:72:8a:60 key-0e07798a956b7d0af 13.115.161.148   Yes

eu-west-3
i-0880b9b3fb210583e (vpc-test-server)             zw-license-key      2021/03/03 29:38:...:df:d4:f1:72:8a:60 key-0c5751dd7cca3ab1d -                No
i-07e4cac8522021aa1 (zwls-prod-paris-vpn)         zw-license-key      2021/03/03 29:38:...:df:d4:f1:72:8a:60 key-0c5751dd7cca3ab1d 35.181.74.247    No
```

I couldn't find zelus-ci.pem, zels-ci-windows.pem, cred-znes-window.pem, and
apg-pdf-test-server.pem. It looked like zelus-ci.pem is the same as
zelus-ci-linux.pem which I found in
[Slack](https://z-emotion.slack.com/archives/GHKQ0TCHL/p1750380062888119?thread_ts=1750379960.954099&cid=GHKQ0TCHL).

Hugo said that i-06697a9e98862806f (znest-api) and i-04c372f5d6c3a4047
(prometheus-runner) are not used any more. So I can ignore the keys I guess.

[This notion page](https://www.notion.so/zemotion/keys-e066d9ce81c14129a97ce9bd52654e0f)
contains several keys.

# 2025-06-30

I compared images that was rendered in a low resolution and resized from the
higher resolution. Resized ones are better. So we decided to use resized
versions and I made a shell script to generate images from the original images.
You can see the difference between them by comparing normal.png and resize.png.
You can find the script in https://github.com/kyle-ze/hyo.

--------------------------------------------------------------------------------

Install WSL ubuntu by `wsl --install Ubuntu`.

Install Docker.destktop from https://www.docker.com/products/docker-desktop/

Run Docker.destop and enable WSL integration otherwise you can't access docker from WSL.
    - Click the gear button on the top right corner of docker.desktop
    - Go to Resouces > WSL integration
    - Enable integration with distro
    - Click 'Apply & restart' at the bottom right
    - shutdown WSL if you're running WSL by `wsl --shutdown`.

Run `group` in WSL. You will see that you are a member of docker group.
`docker ps` should show the list of containers without any error.

Go to the directory that contains the docker compose file and run `docker compose up`.
Run `mysql -h 127.0.0.1 -P 3306 -u root -p` in WSL. It should work.
You have to create a database called 'zemotiondb'. Run `CREATE DATABASE zemotiondb;`
You can also connect to the database using adminer. Connect to localhost:8080.
    - Server: db
    - Username: root
    - Password: <password>
    - Database zemotiondb
The root password is written in docker-compose.yml.

WSL setup

```Bash
# You need install nodejs and npm on WSL otherwise npm on windows will fail to install packages
sudo apt update # without updating the package list `apt install` might be not able to download nodejs and npm
sudo apt install nodejs npm
sudo apt install luit # optional

git clone git@github.com:z-emotion/cloud-ze-frontend.git
git clone git@github.com:z-emotion/cloud-ze-api.git
git clone git@github.com:z-emotion/cloud-ze-backoffice.git
# And run npm install for each directory.

# Copy .env.local to cloud-ze-api
# The root password must be the same as written in cloud-zs-api/.env.local.

# Copy .env to cloud-ze-frontend

# Copy .env to cloud-ze-backoffice

# Install packages
(cd cloud-ze-api && npm install)
(cd cloud-ze-front && npm install)
(cd cloud-ze-backoffice && npm install)

cd cloud-ze-api && npm run start:dev
cd cloud-ze-frontend && npm run dev
cd clied-ze-backoffice && npm run dev
```

Connect to localhost:3000 and see if you can sign up. Now you can see the record
for your account in the database. Change the value of status field to ACTIVE
from READY. Because we can't send a verification mail from localhost. You should
be able to login in with the account you just created.

Change the value of role to ADMIN from USER. Go to backoffice page and login in
with your account. Try to create an account and see if it works.

z-emotion admin page (backoffice) https://di6vhzh0u9tr9.cloudfront.net/

You don't use Docker images that was used is development environment (redis,
mysql, admine, and etc) in production. In production, your application just
accesses to Amazon RDS and ElasticCache. So no docker compose files are need.


In local machine:

Run dockernized mysql server by
`docker run --name mysql-container -e MYSQL_ROOT_PASSWORD=love -d -p 3306:3306 mysql:latest`.
Run `npm run start:dev` in cloud-ze-api directory. It should say "No errors
found" and wait for requests at port 3952. Run `npm run dev` in
cloud-ze-frontend directory. It should say "event - compiled client and server
successfully in 298 ms (668 modules)" and wait for requests at port 3000. You
can open http://localhost:3000.

Backoffice is an adiminstrative site. You can manage users in this site.

- z-emotion admin page (backoffice) https://di6vhzh0u9tr9.cloudfront.net/

# 2025-06-26

Vitarelax project uses aws cli for publishing. So you have to install and
configure it. Just like github cli, aws cli also requires access keys for
security. You can create a access keys following the guide below:
https://docs.aws.amazon.com/IAM/latest/UserGuide/id-credentials-access-keys-update.html#rotating_access_keys_console
You have to save it right after creating it. There's no chance to see it next
time. aws configure will ask your access key and save it to ~/.aws/credential.

# 2025-06-25

Docker.zip Hugo gave to me was a collection of Docker compose files. I opened
that directory with Visual Studio

I couldn't upload Hyosung asset to S3 bucket because I was logged in as
asset-manager which doesn't have a permission. I logged in as kyle which Hugo
created for me and now I'm able to upload files. The root account (id:
649282530217) is shared by all members and each employee has their own IAM user
account. In my case it is kyle. I overrited the files in `/v1/style/test` in S3
and I needed to invalidate cloudfront cache for user to see the changes.

# 2025-06-24

Git for Windows v2.47.0.windows.1 hangs while cloning a repository via SSH.
This issue was resolved in v2.47.0.windows.2.
https://github.com/git-for-windows/git/issues/5199

Git for Windows v2.50.0.windows.1 also hangs while cloning, but apparently it is
a different bug https://github.com/git-for-windows/git/issues/5688
